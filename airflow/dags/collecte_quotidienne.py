# airflow/dags/collecte_quotidienne.py
"""
DAG Airflow pour la collecte quotidienne de donnÃ©es concurrentielles TechPulse
"""

from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from airflow.operators.bash_operator import BashOperator
from datetime import datetime, timedelta
import sys
import os

# Ajouter le dossier scrapers au path Python
sys.path.append('/opt/airflow/scrapers')

# Configuration du DAG
default_args = {
    'owner': 'techpulse-data-team',
    'depends_on_past': False,
    'start_date': datetime(2024, 12, 1),
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
    'catchup': False  # Ne pas rattraper les exÃ©cutions manquÃ©es
}

dag = DAG(
    'techpulse_collecte_quotidienne',
    default_args=default_args,
    description='Collecte quotidienne des donnÃ©es concurrentielles',
    schedule_interval='0 6 * * *',  # Tous les jours Ã  6h du matin
    max_active_runs=1,  # Une seule exÃ©cution Ã  la fois
    tags=['techpulse', 'scraping', 'concurrence']
)

def check_database_connection():
    """VÃ©rifier la connexion Ã  la base de donnÃ©es"""
    import psycopg2
    import logging
    
    logger = logging.getLogger(__name__)
    
    try:
        conn = psycopg2.connect(
            host='postgres',
            port=5432,
            database='techpulse_veille',
            user='techpulse',
            password='techpulse2024'
        )
        
        cursor = conn.cursor()
        cursor.execute("SELECT COUNT(*) FROM sites_concurrents WHERE actif = true;")
        count = cursor.fetchone()[0]
        
        cursor.close()
        conn.close()
        
        logger.info(f"âœ… Connexion DB OK - {count} sites actifs")
        return f"DB_OK_{count}_sites"
        
    except Exception as e:
        logger.error(f"âŒ Erreur connexion DB: {e}")
        raise Exception(f"Connexion DB Ã©chouÃ©e: {e}")

def run_cdiscount_scraping():
    """ExÃ©cuter le scraping Cdiscount avec le scraper final"""
    import logging
    import sys
    import os
    
    # Ajouter le path des scrapers
    sys.path.append('/opt/airflow/scrapers')
    
    logger = logging.getLogger(__name__)
    logger.info("ğŸš€ DÃ©but scraping Cdiscount avec scraper final")
    
    try:
        # Import du scraper final
        from scraper_final_techpulse import TechPulseScraperFinal
        
        # CrÃ©er une instance
        scraper = TechPulseScraperFinal()
        
        # ExÃ©cuter uniquement la collecte Cdiscount
        results = scraper.run_full_collection(target_sites=['cdiscount'])
        
        if results and results.get('cdiscount', {}).get('succes', 0) > 0:
            logger.info(f"âœ… Scraping Cdiscount terminÃ©: {results['cdiscount']['succes']} produits collectÃ©s")
            return "CDISCOUNT_SUCCESS"
        else:
            raise Exception("Aucun produit collectÃ©")
            
    except Exception as e:
        logger.error(f"âŒ Erreur scraping Cdiscount: {e}")
        raise

def run_rueducommerce_scraping():
    """ExÃ©cuter le scraping Rue du Commerce avec le scraper final"""
    import logging
    import sys
    
    sys.path.append('/opt/airflow/scrapers')
    logger = logging.getLogger(__name__)
    logger.info("ğŸš€ DÃ©but scraping Rue du Commerce avec scraper final")
    
    try:
        from scraper_final_techpulse import TechPulseScraperFinal
        scraper = TechPulseScraperFinal()
        results = scraper.run_full_collection(target_sites=['rueducommerce'])
        
        if results and results.get('rueducommerce', {}).get('succes', 0) > 0:
            logger.info(f"âœ… Scraping RDC terminÃ©: {results['rueducommerce']['succes']} produits collectÃ©s")
            return "RDC_SUCCESS"
        else:
            raise Exception("Aucun produit collectÃ©")
            
    except Exception as e:
        logger.error(f"âŒ Erreur scraping RDC: {e}")
        raise

def run_boulanger_scraping():
    """ExÃ©cuter le scraping Boulanger avec le scraper final"""
    import logging
    import sys
    
    sys.path.append('/opt/airflow/scrapers')
    logger = logging.getLogger(__name__)
    logger.info("ğŸš€ DÃ©but scraping Boulanger avec scraper final")
    
    try:
        from scraper_final_techpulse import TechPulseScraperFinal
        scraper = TechPulseScraperFinal()
        results = scraper.run_full_collection(target_sites=['boulanger'])
        
        if results and results.get('boulanger', {}).get('succes', 0) > 0:
            logger.info(f"âœ… Scraping Boulanger terminÃ©: {results['boulanger']['succes']} produits collectÃ©s")
            return "BOULANGER_SUCCESS"
        else:
            raise Exception("Aucun produit collectÃ©")
            
    except Exception as e:
        logger.error(f"âŒ Erreur scraping Boulanger: {e}")
        raise

def generate_daily_report():
    """GÃ©nÃ©rer le rapport quotidien"""
    import psycopg2
    import logging
    from datetime import date
    
    logger = logging.getLogger(__name__)
    logger.info("ğŸ“Š GÃ©nÃ©ration du rapport quotidien")
    
    try:
        conn = psycopg2.connect(
            host='postgres',
            port=5432,
            database='techpulse_veille',
            user='techpulse',
            password='techpulse2024'
        )
        
        cursor = conn.cursor()
        
        # Compter les produits collectÃ©s aujourd'hui
        cursor.execute("""
            SELECT 
                sc.nom_site,
                COUNT(*) as nb_produits
            FROM produits_concurrents pc
            JOIN sites_concurrents sc ON pc.id_site = sc.id_site
            WHERE DATE(pc.date_collecte) = %s
            GROUP BY sc.nom_site
            ORDER BY nb_produits DESC
        """, (date.today(),))
        
        results = cursor.fetchall()
        
        total_produits = sum([row[1] for row in results])
        
        rapport = f"""
ğŸ“Š RAPPORT QUOTIDIEN TECHPULSE - {date.today()}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“ˆ COLLECTE DU JOUR:
â€¢ Total produits collectÃ©s: {total_produits}

ğŸ“‹ DÃ‰TAIL PAR SITE:
"""
        
        for site, nb in results:
            rapport += f"â€¢ {site}: {nb} produits\n"
        
        if not results:
            rapport += "â€¢ Aucune collecte aujourd'hui\n"
        
        rapport += f"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ¤– Rapport gÃ©nÃ©rÃ© automatiquement par Airflow
"""
        
        logger.info(f"ğŸ“Š Rapport gÃ©nÃ©rÃ©:\n{rapport}")
        
        cursor.close()
        conn.close()
        
        return "RAPPORT_OK"
        
    except Exception as e:
        logger.error(f"âŒ Erreur gÃ©nÃ©ration rapport: {e}")
        raise

def send_notification():
    """Envoyer une notification de fin"""
    import logging
    
    logger = logging.getLogger(__name__)
    logger.info("ğŸ“§ Notification: Collecte quotidienne terminÃ©e avec succÃ¨s!")
    
    # Plus tard: envoi email, Slack, etc.
    return "NOTIFICATION_SENT"

# DÃ‰FINITION DES TÃ‚CHES AIRFLOW

# TÃ¢che 1: VÃ©rification prÃ©alable
task_check_db = PythonOperator(
    task_id='check_database_connection',
    python_callable=check_database_connection,
    dag=dag
)

# TÃ¢che 2a: Scraping Cdiscount
task_scraping_cdiscount = PythonOperator(
    task_id='scraping_cdiscount',
    python_callable=run_cdiscount_scraping,
    dag=dag
)

# TÃ¢che 2b: Scraping Rue du Commerce  
task_scraping_rdc = PythonOperator(
    task_id='scraping_rueducommerce',
    python_callable=run_rueducommerce_scraping,
    dag=dag
)

# TÃ¢che 2c: Scraping Boulanger
task_scraping_boulanger = PythonOperator(
    task_id='scraping_boulanger',
    python_callable=run_boulanger_scraping,
    dag=dag
)

# TÃ¢che 3: Rapport quotidien
task_rapport = PythonOperator(
    task_id='generate_daily_report',
    python_callable=generate_daily_report,
    dag=dag
)

# TÃ¢che 4: Notification finale
task_notification = PythonOperator(
    task_id='send_notification',
    python_callable=send_notification,
    dag=dag
)

# TÃ¢che 5: Nettoyage (optionnel)
task_cleanup = BashOperator(
    task_id='cleanup_old_logs',
    bash_command="""
    echo "ğŸ§¹ Nettoyage des anciens logs..."
    # Plus tard: script de nettoyage des donnÃ©es anciennes
    echo "âœ… Nettoyage terminÃ©"
    """,
    dag=dag
)

# DÃ‰FINITION DES DÃ‰PENDANCES (ordre d'exÃ©cution)

# D'abord vÃ©rifier la DB
task_check_db >> [task_scraping_cdiscount, task_scraping_rdc, task_scraping_boulanger]

# Une fois tous les scrapings terminÃ©s, gÃ©nÃ©rer le rapport
[task_scraping_cdiscount, task_scraping_rdc, task_scraping_boulanger] >> task_rapport

# Puis envoyer la notification et nettoyer
task_rapport >> [task_notification, task_cleanup]

# SchÃ©ma des dÃ©pendances:
#
#     check_db
#        |
#   â”Œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”
#   â”‚    â”‚    â”‚
# cdiscount rdc boulanger
#   â”‚    â”‚    â”‚
#   â””â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”˜
#        â”‚
#     rapport
#        |
#   â”Œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”
#   â”‚    â”‚    â”‚
# notification cleanup